# This file was auto-generated by Fern from our API Definition.

import datetime as dt
import typing

from ...core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ...core.request_options import RequestOptions
from ...types.cancel_model_run_response import CancelModelRunResponse
from ...types.model_run import ModelRun
from ...types.project_subset_enum import ProjectSubsetEnum
from .raw_client import AsyncRawRunsClient, RawRunsClient
from .types.list_runs_request_project_subset import ListRunsRequestProjectSubset

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class RunsClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._raw_client = RawRunsClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> RawRunsClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        RawRunsClient
        """
        return self._raw_client

    def list(
        self,
        prompt_id: int,
        version_id: int,
        *,
        ordering: typing.Optional[str] = None,
        parent_model: typing.Optional[int] = None,
        project: typing.Optional[int] = None,
        project_subset: typing.Optional[ListRunsRequestProjectSubset] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[ModelRun]:
        """
        <Card href="https://humansignal.com/goenterprise">
                <img style="pointer-events: none; margin-left: 0px; margin-right: 0px;" src="https://docs.humansignal.com/images/badge.svg" alt="Label Studio Enterprise badge"/>
                <p style="margin-top: 10px; font-size: 14px;">
                    This endpoint is not available in Label Studio Community Edition. [Learn more about Label Studio Enterprise](https://humansignal.com/goenterprise)
                </p>
            </Card>
        Get information (status, metadata, etc) about an existing inference run

        Parameters
        ----------
        prompt_id : int

        version_id : int

        ordering : typing.Optional[str]
            Which field to use when ordering the results.

        parent_model : typing.Optional[int]
            The ID of the parent model for this Inference Run

        project : typing.Optional[int]
            The ID of the project this Inference Run makes predictions on

        project_subset : typing.Optional[ListRunsRequestProjectSubset]
            Defines which tasks are operated on (e.g. HasGT will only operate on tasks with a ground truth annotation, but All will operate on all records)

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[ModelRun]


        Examples
        --------
        from label_studio_sdk import LabelStudio

        client = LabelStudio(
            api_key="YOUR_API_KEY",
        )
        client.prompts.runs.list(
            prompt_id=1,
            version_id=1,
        )
        """
        _response = self._raw_client.list(
            prompt_id,
            version_id,
            ordering=ordering,
            parent_model=parent_model,
            project=project,
            project_subset=project_subset,
            request_options=request_options,
        )
        return _response.data

    def create(
        self,
        prompt_id: int,
        version_id: int,
        *,
        project: int,
        only_missing_predictions: typing.Optional[bool] = OMIT,
        project_subset: typing.Optional[ProjectSubsetEnum] = OMIT,
        job_id: typing.Optional[str] = OMIT,
        total_predictions: typing.Optional[int] = OMIT,
        total_correct_predictions: typing.Optional[int] = OMIT,
        total_tasks: typing.Optional[int] = OMIT,
        predictions_updated_at: typing.Optional[dt.datetime] = OMIT,
        organization: typing.Optional[int] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ModelRun:
        """
        <Card href="https://humansignal.com/goenterprise">
                <img style="pointer-events: none; margin-left: 0px; margin-right: 0px;" src="https://docs.humansignal.com/images/badge.svg" alt="Label Studio Enterprise badge"/>
                <p style="margin-top: 10px; font-size: 14px;">
                    This endpoint is not available in Label Studio Community Edition. [Learn more about Label Studio Enterprise](https://humansignal.com/goenterprise)
                </p>
            </Card>
        Run a prompt inference.

        Parameters
        ----------
        prompt_id : int

        version_id : int

        project : int

        only_missing_predictions : typing.Optional[bool]
            When true, only tasks without successful predictions for this prompt version are submitted for inference.

        project_subset : typing.Optional[ProjectSubsetEnum]

        job_id : typing.Optional[str]
            Job ID for inference job for a ModelRun e.g. Adala job ID

        total_predictions : typing.Optional[int]

        total_correct_predictions : typing.Optional[int]

        total_tasks : typing.Optional[int]

        predictions_updated_at : typing.Optional[dt.datetime]

        organization : typing.Optional[int]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ModelRun


        Examples
        --------
        from label_studio_sdk import LabelStudio

        client = LabelStudio(
            api_key="YOUR_API_KEY",
        )
        client.prompts.runs.create(
            prompt_id=1,
            version_id=1,
            project=1,
        )
        """
        _response = self._raw_client.create(
            prompt_id,
            version_id,
            project=project,
            only_missing_predictions=only_missing_predictions,
            project_subset=project_subset,
            job_id=job_id,
            total_predictions=total_predictions,
            total_correct_predictions=total_correct_predictions,
            total_tasks=total_tasks,
            predictions_updated_at=predictions_updated_at,
            organization=organization,
            request_options=request_options,
        )
        return _response.data

    def cancel(
        self,
        prompt_id: int,
        version_id: int,
        inference_run_id: int,
        *,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> CancelModelRunResponse:
        """
        <Card href="https://humansignal.com/goenterprise">
                <img style="pointer-events: none; margin-left: 0px; margin-right: 0px;" src="https://docs.humansignal.com/images/badge.svg" alt="Label Studio Enterprise badge"/>
                <p style="margin-top: 10px; font-size: 14px;">
                    This endpoint is not available in Label Studio Community Edition. [Learn more about Label Studio Enterprise](https://humansignal.com/goenterprise)
                </p>
            </Card>
        Cancel the inference run for the given api

        Parameters
        ----------
        prompt_id : int

        version_id : int

        inference_run_id : int

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        CancelModelRunResponse


        Examples
        --------
        from label_studio_sdk import LabelStudio

        client = LabelStudio(
            api_key="YOUR_API_KEY",
        )
        client.prompts.runs.cancel(
            prompt_id=1,
            version_id=1,
            inference_run_id=1,
        )
        """
        _response = self._raw_client.cancel(prompt_id, version_id, inference_run_id, request_options=request_options)
        return _response.data


class AsyncRunsClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._raw_client = AsyncRawRunsClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> AsyncRawRunsClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        AsyncRawRunsClient
        """
        return self._raw_client

    async def list(
        self,
        prompt_id: int,
        version_id: int,
        *,
        ordering: typing.Optional[str] = None,
        parent_model: typing.Optional[int] = None,
        project: typing.Optional[int] = None,
        project_subset: typing.Optional[ListRunsRequestProjectSubset] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[ModelRun]:
        """
        <Card href="https://humansignal.com/goenterprise">
                <img style="pointer-events: none; margin-left: 0px; margin-right: 0px;" src="https://docs.humansignal.com/images/badge.svg" alt="Label Studio Enterprise badge"/>
                <p style="margin-top: 10px; font-size: 14px;">
                    This endpoint is not available in Label Studio Community Edition. [Learn more about Label Studio Enterprise](https://humansignal.com/goenterprise)
                </p>
            </Card>
        Get information (status, metadata, etc) about an existing inference run

        Parameters
        ----------
        prompt_id : int

        version_id : int

        ordering : typing.Optional[str]
            Which field to use when ordering the results.

        parent_model : typing.Optional[int]
            The ID of the parent model for this Inference Run

        project : typing.Optional[int]
            The ID of the project this Inference Run makes predictions on

        project_subset : typing.Optional[ListRunsRequestProjectSubset]
            Defines which tasks are operated on (e.g. HasGT will only operate on tasks with a ground truth annotation, but All will operate on all records)

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[ModelRun]


        Examples
        --------
        import asyncio

        from label_studio_sdk import AsyncLabelStudio

        client = AsyncLabelStudio(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.runs.list(
                prompt_id=1,
                version_id=1,
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.list(
            prompt_id,
            version_id,
            ordering=ordering,
            parent_model=parent_model,
            project=project,
            project_subset=project_subset,
            request_options=request_options,
        )
        return _response.data

    async def create(
        self,
        prompt_id: int,
        version_id: int,
        *,
        project: int,
        only_missing_predictions: typing.Optional[bool] = OMIT,
        project_subset: typing.Optional[ProjectSubsetEnum] = OMIT,
        job_id: typing.Optional[str] = OMIT,
        total_predictions: typing.Optional[int] = OMIT,
        total_correct_predictions: typing.Optional[int] = OMIT,
        total_tasks: typing.Optional[int] = OMIT,
        predictions_updated_at: typing.Optional[dt.datetime] = OMIT,
        organization: typing.Optional[int] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ModelRun:
        """
        <Card href="https://humansignal.com/goenterprise">
                <img style="pointer-events: none; margin-left: 0px; margin-right: 0px;" src="https://docs.humansignal.com/images/badge.svg" alt="Label Studio Enterprise badge"/>
                <p style="margin-top: 10px; font-size: 14px;">
                    This endpoint is not available in Label Studio Community Edition. [Learn more about Label Studio Enterprise](https://humansignal.com/goenterprise)
                </p>
            </Card>
        Run a prompt inference.

        Parameters
        ----------
        prompt_id : int

        version_id : int

        project : int

        only_missing_predictions : typing.Optional[bool]
            When true, only tasks without successful predictions for this prompt version are submitted for inference.

        project_subset : typing.Optional[ProjectSubsetEnum]

        job_id : typing.Optional[str]
            Job ID for inference job for a ModelRun e.g. Adala job ID

        total_predictions : typing.Optional[int]

        total_correct_predictions : typing.Optional[int]

        total_tasks : typing.Optional[int]

        predictions_updated_at : typing.Optional[dt.datetime]

        organization : typing.Optional[int]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ModelRun


        Examples
        --------
        import asyncio

        from label_studio_sdk import AsyncLabelStudio

        client = AsyncLabelStudio(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.runs.create(
                prompt_id=1,
                version_id=1,
                project=1,
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.create(
            prompt_id,
            version_id,
            project=project,
            only_missing_predictions=only_missing_predictions,
            project_subset=project_subset,
            job_id=job_id,
            total_predictions=total_predictions,
            total_correct_predictions=total_correct_predictions,
            total_tasks=total_tasks,
            predictions_updated_at=predictions_updated_at,
            organization=organization,
            request_options=request_options,
        )
        return _response.data

    async def cancel(
        self,
        prompt_id: int,
        version_id: int,
        inference_run_id: int,
        *,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> CancelModelRunResponse:
        """
        <Card href="https://humansignal.com/goenterprise">
                <img style="pointer-events: none; margin-left: 0px; margin-right: 0px;" src="https://docs.humansignal.com/images/badge.svg" alt="Label Studio Enterprise badge"/>
                <p style="margin-top: 10px; font-size: 14px;">
                    This endpoint is not available in Label Studio Community Edition. [Learn more about Label Studio Enterprise](https://humansignal.com/goenterprise)
                </p>
            </Card>
        Cancel the inference run for the given api

        Parameters
        ----------
        prompt_id : int

        version_id : int

        inference_run_id : int

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        CancelModelRunResponse


        Examples
        --------
        import asyncio

        from label_studio_sdk import AsyncLabelStudio

        client = AsyncLabelStudio(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.runs.cancel(
                prompt_id=1,
                version_id=1,
                inference_run_id=1,
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.cancel(
            prompt_id, version_id, inference_run_id, request_options=request_options
        )
        return _response.data
