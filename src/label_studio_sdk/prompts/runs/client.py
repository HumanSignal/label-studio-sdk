# This file was auto-generated by Fern from our API Definition.

import datetime as dt
import typing

from ...core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ...core.request_options import RequestOptions
from ...types.inference_run import InferenceRun
from ...types.inference_run_created_by import InferenceRunCreatedBy
from ...types.inference_run_organization import InferenceRunOrganization
from ...types.inference_run_project_subset import InferenceRunProjectSubset
from ...types.inference_run_status import InferenceRunStatus
from .raw_client import AsyncRawRunsClient, RawRunsClient
from .types.runs_list_request_project_subset import RunsListRequestProjectSubset

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class RunsClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._raw_client = RawRunsClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> RawRunsClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        RawRunsClient
        """
        return self._raw_client

    def list(
        self,
        id: int,
        version_id: int,
        *,
        project: int,
        project_subset: RunsListRequestProjectSubset,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> InferenceRun:
        """
        Get information (status, etadata, etc) about an existing inference run

        Parameters
        ----------
        id : int
            Prompt ID

        version_id : int
            Prompt Version ID

        project : int
            The ID of the project that this Interence Run makes predictions on

        project_subset : RunsListRequestProjectSubset
            Defines which tasks are operated on (e.g. HasGT will only operate on tasks with a ground truth annotation, but All will operate on all records)

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        InferenceRun
            Success

        Examples
        --------
        from label_studio_sdk import LabelStudio

        client = LabelStudio(
            api_key="YOUR_API_KEY",
        )
        client.prompts.runs.list(
            id=1,
            version_id=1,
            project=1,
            project_subset="All",
        )
        """
        _response = self._raw_client.list(
            id, version_id, project=project, project_subset=project_subset, request_options=request_options
        )
        return _response.data

    def create(
        self,
        id: int,
        version_id: int,
        *,
        project: int,
        project_subset: InferenceRunProjectSubset,
        organization: typing.Optional[InferenceRunOrganization] = OMIT,
        model_version: typing.Optional[int] = OMIT,
        created_by: typing.Optional[InferenceRunCreatedBy] = OMIT,
        status: typing.Optional[InferenceRunStatus] = OMIT,
        job_id: typing.Optional[str] = OMIT,
        created_at: typing.Optional[dt.datetime] = OMIT,
        triggered_at: typing.Optional[dt.datetime] = OMIT,
        predictions_updated_at: typing.Optional[dt.datetime] = OMIT,
        completed_at: typing.Optional[dt.datetime] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> InferenceRun:
        """
        Run a prompt inference.

        Parameters
        ----------
        id : int
            Prompt ID

        version_id : int
            Prompt Version ID

        project : int

        project_subset : InferenceRunProjectSubset

        organization : typing.Optional[InferenceRunOrganization]

        model_version : typing.Optional[int]

        created_by : typing.Optional[InferenceRunCreatedBy]

        status : typing.Optional[InferenceRunStatus]

        job_id : typing.Optional[str]

        created_at : typing.Optional[dt.datetime]

        triggered_at : typing.Optional[dt.datetime]

        predictions_updated_at : typing.Optional[dt.datetime]

        completed_at : typing.Optional[dt.datetime]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        InferenceRun


        Examples
        --------
        from label_studio_sdk import LabelStudio

        client = LabelStudio(
            api_key="YOUR_API_KEY",
        )
        client.prompts.runs.create(
            id=1,
            version_id=1,
            project=1,
            project_subset="All",
        )
        """
        _response = self._raw_client.create(
            id,
            version_id,
            project=project,
            project_subset=project_subset,
            organization=organization,
            model_version=model_version,
            created_by=created_by,
            status=status,
            job_id=job_id,
            created_at=created_at,
            triggered_at=triggered_at,
            predictions_updated_at=predictions_updated_at,
            completed_at=completed_at,
            request_options=request_options,
        )
        return _response.data


class AsyncRunsClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._raw_client = AsyncRawRunsClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> AsyncRawRunsClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        AsyncRawRunsClient
        """
        return self._raw_client

    async def list(
        self,
        id: int,
        version_id: int,
        *,
        project: int,
        project_subset: RunsListRequestProjectSubset,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> InferenceRun:
        """
        Get information (status, etadata, etc) about an existing inference run

        Parameters
        ----------
        id : int
            Prompt ID

        version_id : int
            Prompt Version ID

        project : int
            The ID of the project that this Interence Run makes predictions on

        project_subset : RunsListRequestProjectSubset
            Defines which tasks are operated on (e.g. HasGT will only operate on tasks with a ground truth annotation, but All will operate on all records)

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        InferenceRun
            Success

        Examples
        --------
        import asyncio

        from label_studio_sdk import AsyncLabelStudio

        client = AsyncLabelStudio(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.runs.list(
                id=1,
                version_id=1,
                project=1,
                project_subset="All",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.list(
            id, version_id, project=project, project_subset=project_subset, request_options=request_options
        )
        return _response.data

    async def create(
        self,
        id: int,
        version_id: int,
        *,
        project: int,
        project_subset: InferenceRunProjectSubset,
        organization: typing.Optional[InferenceRunOrganization] = OMIT,
        model_version: typing.Optional[int] = OMIT,
        created_by: typing.Optional[InferenceRunCreatedBy] = OMIT,
        status: typing.Optional[InferenceRunStatus] = OMIT,
        job_id: typing.Optional[str] = OMIT,
        created_at: typing.Optional[dt.datetime] = OMIT,
        triggered_at: typing.Optional[dt.datetime] = OMIT,
        predictions_updated_at: typing.Optional[dt.datetime] = OMIT,
        completed_at: typing.Optional[dt.datetime] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> InferenceRun:
        """
        Run a prompt inference.

        Parameters
        ----------
        id : int
            Prompt ID

        version_id : int
            Prompt Version ID

        project : int

        project_subset : InferenceRunProjectSubset

        organization : typing.Optional[InferenceRunOrganization]

        model_version : typing.Optional[int]

        created_by : typing.Optional[InferenceRunCreatedBy]

        status : typing.Optional[InferenceRunStatus]

        job_id : typing.Optional[str]

        created_at : typing.Optional[dt.datetime]

        triggered_at : typing.Optional[dt.datetime]

        predictions_updated_at : typing.Optional[dt.datetime]

        completed_at : typing.Optional[dt.datetime]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        InferenceRun


        Examples
        --------
        import asyncio

        from label_studio_sdk import AsyncLabelStudio

        client = AsyncLabelStudio(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.runs.create(
                id=1,
                version_id=1,
                project=1,
                project_subset="All",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.create(
            id,
            version_id,
            project=project,
            project_subset=project_subset,
            organization=organization,
            model_version=model_version,
            created_by=created_by,
            status=status,
            job_id=job_id,
            created_at=created_at,
            triggered_at=triggered_at,
            predictions_updated_at=predictions_updated_at,
            completed_at=completed_at,
            request_options=request_options,
        )
        return _response.data
