# This file was auto-generated by Fern from our API Definition.

import typing
from ...core.client_wrapper import SyncClientWrapper
from .types.runs_list_request_project_subset import RunsListRequestProjectSubset
from ...core.request_options import RequestOptions
from ...types.model_run import ModelRun
from ...core.jsonable_encoder import jsonable_encoder
from ...core.unchecked_base_model import construct_type
from json.decoder import JSONDecodeError
from ...core.api_error import ApiError
import datetime as dt
from ...types.project_subset_enum import ProjectSubsetEnum
from ...core.client_wrapper import AsyncClientWrapper

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class RunsClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def list(
        self,
        prompt_id: int,
        version_id: int,
        *,
        ordering: typing.Optional[str] = None,
        parent_model: typing.Optional[int] = None,
        project: typing.Optional[int] = None,
        project_subset: typing.Optional[RunsListRequestProjectSubset] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[ModelRun]:
        """
        Get information (status, metadata, etc) about an existing inference run

        Parameters
        ----------
        prompt_id : int

        version_id : int

        ordering : typing.Optional[str]
            Which field to use when ordering the results.

        parent_model : typing.Optional[int]
            The ID of the parent model for this Inference Run

        project : typing.Optional[int]
            The ID of the project this Inference Run makes predictions on

        project_subset : typing.Optional[RunsListRequestProjectSubset]
            Defines which tasks are operated on (e.g. HasGT will only operate on tasks with a ground truth annotation, but All will operate on all records)

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[ModelRun]


        Examples
        --------
        from label_studio_sdk import LabelStudio

        client = LabelStudio(
            api_key="YOUR_API_KEY",
        )
        client.prompts.runs.list(
            prompt_id=1,
            version_id=1,
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/prompts/{jsonable_encoder(prompt_id)}/versions/{jsonable_encoder(version_id)}/inference-runs",
            method="GET",
            params={
                "ordering": ordering,
                "parent_model": parent_model,
                "project": project,
                "project_subset": project_subset,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[ModelRun],
                    construct_type(
                        type_=typing.List[ModelRun],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create(
        self,
        prompt_id: int,
        version_id: int,
        *,
        project: int,
        job_id: typing.Optional[str] = OMIT,
        organization: typing.Optional[int] = OMIT,
        predictions_updated_at: typing.Optional[dt.datetime] = OMIT,
        project_subset: typing.Optional[ProjectSubsetEnum] = OMIT,
        total_correct_predictions: typing.Optional[int] = OMIT,
        total_predictions: typing.Optional[int] = OMIT,
        total_tasks: typing.Optional[int] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ModelRun:
        """
        Run a prompt inference.

        Parameters
        ----------
        prompt_id : int

        version_id : int

        project : int

        job_id : typing.Optional[str]
            Job ID for inference job for a ModelRun e.g. Adala job ID

        organization : typing.Optional[int]

        predictions_updated_at : typing.Optional[dt.datetime]

        project_subset : typing.Optional[ProjectSubsetEnum]

        total_correct_predictions : typing.Optional[int]

        total_predictions : typing.Optional[int]

        total_tasks : typing.Optional[int]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ModelRun


        Examples
        --------
        from label_studio_sdk import LabelStudio

        client = LabelStudio(
            api_key="YOUR_API_KEY",
        )
        client.prompts.runs.create(
            prompt_id=1,
            version_id=1,
            project=1,
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/prompts/{jsonable_encoder(prompt_id)}/versions/{jsonable_encoder(version_id)}/inference-runs",
            method="POST",
            json={
                "job_id": job_id,
                "organization": organization,
                "predictions_updated_at": predictions_updated_at,
                "project": project,
                "project_subset": project_subset,
                "total_correct_predictions": total_correct_predictions,
                "total_predictions": total_predictions,
                "total_tasks": total_tasks,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ModelRun,
                    construct_type(
                        type_=ModelRun,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncRunsClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def list(
        self,
        prompt_id: int,
        version_id: int,
        *,
        ordering: typing.Optional[str] = None,
        parent_model: typing.Optional[int] = None,
        project: typing.Optional[int] = None,
        project_subset: typing.Optional[RunsListRequestProjectSubset] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[ModelRun]:
        """
        Get information (status, metadata, etc) about an existing inference run

        Parameters
        ----------
        prompt_id : int

        version_id : int

        ordering : typing.Optional[str]
            Which field to use when ordering the results.

        parent_model : typing.Optional[int]
            The ID of the parent model for this Inference Run

        project : typing.Optional[int]
            The ID of the project this Inference Run makes predictions on

        project_subset : typing.Optional[RunsListRequestProjectSubset]
            Defines which tasks are operated on (e.g. HasGT will only operate on tasks with a ground truth annotation, but All will operate on all records)

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[ModelRun]


        Examples
        --------
        import asyncio

        from label_studio_sdk import AsyncLabelStudio

        client = AsyncLabelStudio(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.runs.list(
                prompt_id=1,
                version_id=1,
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/prompts/{jsonable_encoder(prompt_id)}/versions/{jsonable_encoder(version_id)}/inference-runs",
            method="GET",
            params={
                "ordering": ordering,
                "parent_model": parent_model,
                "project": project,
                "project_subset": project_subset,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[ModelRun],
                    construct_type(
                        type_=typing.List[ModelRun],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create(
        self,
        prompt_id: int,
        version_id: int,
        *,
        project: int,
        job_id: typing.Optional[str] = OMIT,
        organization: typing.Optional[int] = OMIT,
        predictions_updated_at: typing.Optional[dt.datetime] = OMIT,
        project_subset: typing.Optional[ProjectSubsetEnum] = OMIT,
        total_correct_predictions: typing.Optional[int] = OMIT,
        total_predictions: typing.Optional[int] = OMIT,
        total_tasks: typing.Optional[int] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ModelRun:
        """
        Run a prompt inference.

        Parameters
        ----------
        prompt_id : int

        version_id : int

        project : int

        job_id : typing.Optional[str]
            Job ID for inference job for a ModelRun e.g. Adala job ID

        organization : typing.Optional[int]

        predictions_updated_at : typing.Optional[dt.datetime]

        project_subset : typing.Optional[ProjectSubsetEnum]

        total_correct_predictions : typing.Optional[int]

        total_predictions : typing.Optional[int]

        total_tasks : typing.Optional[int]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ModelRun


        Examples
        --------
        import asyncio

        from label_studio_sdk import AsyncLabelStudio

        client = AsyncLabelStudio(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.prompts.runs.create(
                prompt_id=1,
                version_id=1,
                project=1,
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/prompts/{jsonable_encoder(prompt_id)}/versions/{jsonable_encoder(version_id)}/inference-runs",
            method="POST",
            json={
                "job_id": job_id,
                "organization": organization,
                "predictions_updated_at": predictions_updated_at,
                "project": project,
                "project_subset": project_subset,
                "total_correct_predictions": total_correct_predictions,
                "total_predictions": total_predictions,
                "total_tasks": total_tasks,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    ModelRun,
                    construct_type(
                        type_=ModelRun,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)
