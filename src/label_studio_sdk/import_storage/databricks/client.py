# This file was auto-generated by Fern from our API Definition.

import typing
from ...core.client_wrapper import SyncClientWrapper
from ...core.request_options import RequestOptions
from ...types.databricks_import_storage import DatabricksImportStorage
from ...core.unchecked_base_model import construct_type
from json.decoder import JSONDecodeError
from ...core.api_error import ApiError
import datetime as dt
from ...types.status_c5a_enum import StatusC5AEnum
from ...core.jsonable_encoder import jsonable_encoder
from ...core.client_wrapper import AsyncClientWrapper

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class DatabricksClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def list(
        self,
        *,
        ordering: typing.Optional[str] = None,
        project: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[DatabricksImportStorage]:
        """
        Get list of all Databricks Files import storage connections.

        Parameters
        ----------
        ordering : typing.Optional[str]
            Which field to use when ordering the results.

        project : typing.Optional[int]
            Project ID

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[DatabricksImportStorage]


        Examples
        --------
        from label_studio_sdk import LabelStudio

        client = LabelStudio(
            api_key="YOUR_API_KEY",
        )
        client.import_storage.databricks.list()
        """
        _response = self._client_wrapper.httpx_client.request(
            "api/storages/databricks/",
            method="GET",
            params={
                "ordering": ordering,
                "project": project,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[DatabricksImportStorage],
                    construct_type(
                        type_=typing.List[DatabricksImportStorage],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create(
        self,
        *,
        catalog: str,
        host: str,
        project: int,
        schema: str,
        volume: str,
        description: typing.Optional[str] = OMIT,
        last_sync: typing.Optional[dt.datetime] = OMIT,
        last_sync_count: typing.Optional[int] = OMIT,
        last_sync_job: typing.Optional[str] = OMIT,
        meta: typing.Optional[typing.Optional[typing.Any]] = OMIT,
        prefix: typing.Optional[str] = OMIT,
        presign: typing.Optional[bool] = OMIT,
        presign_ttl: typing.Optional[int] = OMIT,
        recursive_scan: typing.Optional[bool] = OMIT,
        regex_filter: typing.Optional[str] = OMIT,
        request_timeout_s: typing.Optional[int] = OMIT,
        status: typing.Optional[StatusC5AEnum] = OMIT,
        stream_chunk_bytes: typing.Optional[int] = OMIT,
        synchronizable: typing.Optional[bool] = OMIT,
        title: typing.Optional[str] = OMIT,
        token: typing.Optional[str] = OMIT,
        traceback: typing.Optional[str] = OMIT,
        use_blob_urls: typing.Optional[bool] = OMIT,
        verify_tls: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> DatabricksImportStorage:
        """
        Create a Databricks Files import storage connection.

        Parameters
        ----------
        catalog : str
            UC catalog name

        host : str
            Databricks workspace base URL (https://...)

        project : int
            A unique integer value identifying this project.

        schema : str
            UC schema name

        volume : str
            UC volume name

        description : typing.Optional[str]
            Cloud storage description

        last_sync : typing.Optional[dt.datetime]
            Last sync finished time

        last_sync_count : typing.Optional[int]
            Count of tasks synced last time

        last_sync_job : typing.Optional[str]
            Last sync job ID

        meta : typing.Optional[typing.Optional[typing.Any]]

        prefix : typing.Optional[str]
            Path under the volume

        presign : typing.Optional[bool]
            Presign not supported; always proxied

        presign_ttl : typing.Optional[int]
            Unused for Databricks; kept for compatibility

        recursive_scan : typing.Optional[bool]
            Perform recursive scan

        regex_filter : typing.Optional[str]
            Regex for filtering objects

        request_timeout_s : typing.Optional[int]

        status : typing.Optional[StatusC5AEnum]

        stream_chunk_bytes : typing.Optional[int]

        synchronizable : typing.Optional[bool]

        title : typing.Optional[str]
            Cloud storage title

        token : typing.Optional[str]

        traceback : typing.Optional[str]
            Traceback report for the last failed sync

        use_blob_urls : typing.Optional[bool]
            Generate blob URLs in tasks

        verify_tls : typing.Optional[bool]
            Verify TLS certificates

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        DatabricksImportStorage


        Examples
        --------
        from label_studio_sdk import LabelStudio

        client = LabelStudio(
            api_key="YOUR_API_KEY",
        )
        client.import_storage.databricks.create(
            catalog="catalog",
            host="host",
            project=1,
            schema="schema",
            volume="volume",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "api/storages/databricks/",
            method="POST",
            json={
                "catalog": catalog,
                "description": description,
                "host": host,
                "last_sync": last_sync,
                "last_sync_count": last_sync_count,
                "last_sync_job": last_sync_job,
                "meta": meta,
                "prefix": prefix,
                "presign": presign,
                "presign_ttl": presign_ttl,
                "project": project,
                "recursive_scan": recursive_scan,
                "regex_filter": regex_filter,
                "request_timeout_s": request_timeout_s,
                "schema": schema,
                "status": status,
                "stream_chunk_bytes": stream_chunk_bytes,
                "synchronizable": synchronizable,
                "title": title,
                "token": token,
                "traceback": traceback,
                "use_blob_urls": use_blob_urls,
                "verify_tls": verify_tls,
                "volume": volume,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    DatabricksImportStorage,
                    construct_type(
                        type_=DatabricksImportStorage,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def validate(
        self,
        *,
        catalog: str,
        host: str,
        project: int,
        schema: str,
        volume: str,
        description: typing.Optional[str] = OMIT,
        last_sync: typing.Optional[dt.datetime] = OMIT,
        last_sync_count: typing.Optional[int] = OMIT,
        last_sync_job: typing.Optional[str] = OMIT,
        meta: typing.Optional[typing.Optional[typing.Any]] = OMIT,
        prefix: typing.Optional[str] = OMIT,
        presign: typing.Optional[bool] = OMIT,
        presign_ttl: typing.Optional[int] = OMIT,
        recursive_scan: typing.Optional[bool] = OMIT,
        regex_filter: typing.Optional[str] = OMIT,
        request_timeout_s: typing.Optional[int] = OMIT,
        status: typing.Optional[StatusC5AEnum] = OMIT,
        stream_chunk_bytes: typing.Optional[int] = OMIT,
        synchronizable: typing.Optional[bool] = OMIT,
        title: typing.Optional[str] = OMIT,
        token: typing.Optional[str] = OMIT,
        traceback: typing.Optional[str] = OMIT,
        use_blob_urls: typing.Optional[bool] = OMIT,
        verify_tls: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> None:
        """
        Validate a specific Databricks Files import storage connection.

        Parameters
        ----------
        catalog : str
            UC catalog name

        host : str
            Databricks workspace base URL (https://...)

        project : int
            A unique integer value identifying this project.

        schema : str
            UC schema name

        volume : str
            UC volume name

        description : typing.Optional[str]
            Cloud storage description

        last_sync : typing.Optional[dt.datetime]
            Last sync finished time

        last_sync_count : typing.Optional[int]
            Count of tasks synced last time

        last_sync_job : typing.Optional[str]
            Last sync job ID

        meta : typing.Optional[typing.Optional[typing.Any]]

        prefix : typing.Optional[str]
            Path under the volume

        presign : typing.Optional[bool]
            Presign not supported; always proxied

        presign_ttl : typing.Optional[int]
            Unused for Databricks; kept for compatibility

        recursive_scan : typing.Optional[bool]
            Perform recursive scan

        regex_filter : typing.Optional[str]
            Regex for filtering objects

        request_timeout_s : typing.Optional[int]

        status : typing.Optional[StatusC5AEnum]

        stream_chunk_bytes : typing.Optional[int]

        synchronizable : typing.Optional[bool]

        title : typing.Optional[str]
            Cloud storage title

        token : typing.Optional[str]

        traceback : typing.Optional[str]
            Traceback report for the last failed sync

        use_blob_urls : typing.Optional[bool]
            Generate blob URLs in tasks

        verify_tls : typing.Optional[bool]
            Verify TLS certificates

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from label_studio_sdk import LabelStudio

        client = LabelStudio(
            api_key="YOUR_API_KEY",
        )
        client.import_storage.databricks.validate(
            catalog="catalog",
            host="host",
            project=1,
            schema="schema",
            volume="volume",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "api/storages/databricks/validate",
            method="POST",
            json={
                "catalog": catalog,
                "description": description,
                "host": host,
                "last_sync": last_sync,
                "last_sync_count": last_sync_count,
                "last_sync_job": last_sync_job,
                "meta": meta,
                "prefix": prefix,
                "presign": presign,
                "presign_ttl": presign_ttl,
                "project": project,
                "recursive_scan": recursive_scan,
                "regex_filter": regex_filter,
                "request_timeout_s": request_timeout_s,
                "schema": schema,
                "status": status,
                "stream_chunk_bytes": stream_chunk_bytes,
                "synchronizable": synchronizable,
                "title": title,
                "token": token,
                "traceback": traceback,
                "use_blob_urls": use_blob_urls,
                "verify_tls": verify_tls,
                "volume": volume,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def get(self, id: int, *, request_options: typing.Optional[RequestOptions] = None) -> DatabricksImportStorage:
        """
        Get a specific Databricks Files import storage connection.

        Parameters
        ----------
        id : int

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        DatabricksImportStorage


        Examples
        --------
        from label_studio_sdk import LabelStudio

        client = LabelStudio(
            api_key="YOUR_API_KEY",
        )
        client.import_storage.databricks.get(
            id=1,
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/storages/databricks/{jsonable_encoder(id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    DatabricksImportStorage,
                    construct_type(
                        type_=DatabricksImportStorage,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def delete(self, id: int, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Delete a specific Databricks Files import storage connection.

        Parameters
        ----------
        id : int

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from label_studio_sdk import LabelStudio

        client = LabelStudio(
            api_key="YOUR_API_KEY",
        )
        client.import_storage.databricks.delete(
            id=1,
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/storages/databricks/{jsonable_encoder(id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def update(
        self,
        id: int,
        *,
        catalog: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        host: typing.Optional[str] = OMIT,
        last_sync: typing.Optional[dt.datetime] = OMIT,
        last_sync_count: typing.Optional[int] = OMIT,
        last_sync_job: typing.Optional[str] = OMIT,
        meta: typing.Optional[typing.Optional[typing.Any]] = OMIT,
        prefix: typing.Optional[str] = OMIT,
        presign: typing.Optional[bool] = OMIT,
        presign_ttl: typing.Optional[int] = OMIT,
        project: typing.Optional[int] = OMIT,
        recursive_scan: typing.Optional[bool] = OMIT,
        regex_filter: typing.Optional[str] = OMIT,
        request_timeout_s: typing.Optional[int] = OMIT,
        schema: typing.Optional[str] = OMIT,
        status: typing.Optional[StatusC5AEnum] = OMIT,
        stream_chunk_bytes: typing.Optional[int] = OMIT,
        synchronizable: typing.Optional[bool] = OMIT,
        title: typing.Optional[str] = OMIT,
        token: typing.Optional[str] = OMIT,
        traceback: typing.Optional[str] = OMIT,
        use_blob_urls: typing.Optional[bool] = OMIT,
        verify_tls: typing.Optional[bool] = OMIT,
        volume: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> DatabricksImportStorage:
        """
        Update a specific Databricks Files import storage connection.

        Parameters
        ----------
        id : int

        catalog : typing.Optional[str]
            UC catalog name

        description : typing.Optional[str]
            Cloud storage description

        host : typing.Optional[str]
            Databricks workspace base URL (https://...)

        last_sync : typing.Optional[dt.datetime]
            Last sync finished time

        last_sync_count : typing.Optional[int]
            Count of tasks synced last time

        last_sync_job : typing.Optional[str]
            Last sync job ID

        meta : typing.Optional[typing.Optional[typing.Any]]

        prefix : typing.Optional[str]
            Path under the volume

        presign : typing.Optional[bool]
            Presign not supported; always proxied

        presign_ttl : typing.Optional[int]
            Unused for Databricks; kept for compatibility

        project : typing.Optional[int]
            A unique integer value identifying this project.

        recursive_scan : typing.Optional[bool]
            Perform recursive scan

        regex_filter : typing.Optional[str]
            Regex for filtering objects

        request_timeout_s : typing.Optional[int]

        schema : typing.Optional[str]
            UC schema name

        status : typing.Optional[StatusC5AEnum]

        stream_chunk_bytes : typing.Optional[int]

        synchronizable : typing.Optional[bool]

        title : typing.Optional[str]
            Cloud storage title

        token : typing.Optional[str]

        traceback : typing.Optional[str]
            Traceback report for the last failed sync

        use_blob_urls : typing.Optional[bool]
            Generate blob URLs in tasks

        verify_tls : typing.Optional[bool]
            Verify TLS certificates

        volume : typing.Optional[str]
            UC volume name

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        DatabricksImportStorage


        Examples
        --------
        from label_studio_sdk import LabelStudio

        client = LabelStudio(
            api_key="YOUR_API_KEY",
        )
        client.import_storage.databricks.update(
            id=1,
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/storages/databricks/{jsonable_encoder(id)}",
            method="PATCH",
            json={
                "catalog": catalog,
                "description": description,
                "host": host,
                "last_sync": last_sync,
                "last_sync_count": last_sync_count,
                "last_sync_job": last_sync_job,
                "meta": meta,
                "prefix": prefix,
                "presign": presign,
                "presign_ttl": presign_ttl,
                "project": project,
                "recursive_scan": recursive_scan,
                "regex_filter": regex_filter,
                "request_timeout_s": request_timeout_s,
                "schema": schema,
                "status": status,
                "stream_chunk_bytes": stream_chunk_bytes,
                "synchronizable": synchronizable,
                "title": title,
                "token": token,
                "traceback": traceback,
                "use_blob_urls": use_blob_urls,
                "verify_tls": verify_tls,
                "volume": volume,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    DatabricksImportStorage,
                    construct_type(
                        type_=DatabricksImportStorage,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def sync(self, id: int, *, request_options: typing.Optional[RequestOptions] = None) -> DatabricksImportStorage:
        """
        Sync tasks from a Databricks Files import storage.

        Parameters
        ----------
        id : int

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        DatabricksImportStorage


        Examples
        --------
        from label_studio_sdk import LabelStudio

        client = LabelStudio(
            api_key="YOUR_API_KEY",
        )
        client.import_storage.databricks.sync(
            id=1,
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/storages/databricks/{jsonable_encoder(id)}/sync",
            method="POST",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    DatabricksImportStorage,
                    construct_type(
                        type_=DatabricksImportStorage,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncDatabricksClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def list(
        self,
        *,
        ordering: typing.Optional[str] = None,
        project: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[DatabricksImportStorage]:
        """
        Get list of all Databricks Files import storage connections.

        Parameters
        ----------
        ordering : typing.Optional[str]
            Which field to use when ordering the results.

        project : typing.Optional[int]
            Project ID

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[DatabricksImportStorage]


        Examples
        --------
        import asyncio

        from label_studio_sdk import AsyncLabelStudio

        client = AsyncLabelStudio(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.import_storage.databricks.list()


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "api/storages/databricks/",
            method="GET",
            params={
                "ordering": ordering,
                "project": project,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    typing.List[DatabricksImportStorage],
                    construct_type(
                        type_=typing.List[DatabricksImportStorage],  # type: ignore
                        object_=_response.json(),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create(
        self,
        *,
        catalog: str,
        host: str,
        project: int,
        schema: str,
        volume: str,
        description: typing.Optional[str] = OMIT,
        last_sync: typing.Optional[dt.datetime] = OMIT,
        last_sync_count: typing.Optional[int] = OMIT,
        last_sync_job: typing.Optional[str] = OMIT,
        meta: typing.Optional[typing.Optional[typing.Any]] = OMIT,
        prefix: typing.Optional[str] = OMIT,
        presign: typing.Optional[bool] = OMIT,
        presign_ttl: typing.Optional[int] = OMIT,
        recursive_scan: typing.Optional[bool] = OMIT,
        regex_filter: typing.Optional[str] = OMIT,
        request_timeout_s: typing.Optional[int] = OMIT,
        status: typing.Optional[StatusC5AEnum] = OMIT,
        stream_chunk_bytes: typing.Optional[int] = OMIT,
        synchronizable: typing.Optional[bool] = OMIT,
        title: typing.Optional[str] = OMIT,
        token: typing.Optional[str] = OMIT,
        traceback: typing.Optional[str] = OMIT,
        use_blob_urls: typing.Optional[bool] = OMIT,
        verify_tls: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> DatabricksImportStorage:
        """
        Create a Databricks Files import storage connection.

        Parameters
        ----------
        catalog : str
            UC catalog name

        host : str
            Databricks workspace base URL (https://...)

        project : int
            A unique integer value identifying this project.

        schema : str
            UC schema name

        volume : str
            UC volume name

        description : typing.Optional[str]
            Cloud storage description

        last_sync : typing.Optional[dt.datetime]
            Last sync finished time

        last_sync_count : typing.Optional[int]
            Count of tasks synced last time

        last_sync_job : typing.Optional[str]
            Last sync job ID

        meta : typing.Optional[typing.Optional[typing.Any]]

        prefix : typing.Optional[str]
            Path under the volume

        presign : typing.Optional[bool]
            Presign not supported; always proxied

        presign_ttl : typing.Optional[int]
            Unused for Databricks; kept for compatibility

        recursive_scan : typing.Optional[bool]
            Perform recursive scan

        regex_filter : typing.Optional[str]
            Regex for filtering objects

        request_timeout_s : typing.Optional[int]

        status : typing.Optional[StatusC5AEnum]

        stream_chunk_bytes : typing.Optional[int]

        synchronizable : typing.Optional[bool]

        title : typing.Optional[str]
            Cloud storage title

        token : typing.Optional[str]

        traceback : typing.Optional[str]
            Traceback report for the last failed sync

        use_blob_urls : typing.Optional[bool]
            Generate blob URLs in tasks

        verify_tls : typing.Optional[bool]
            Verify TLS certificates

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        DatabricksImportStorage


        Examples
        --------
        import asyncio

        from label_studio_sdk import AsyncLabelStudio

        client = AsyncLabelStudio(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.import_storage.databricks.create(
                catalog="catalog",
                host="host",
                project=1,
                schema="schema",
                volume="volume",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "api/storages/databricks/",
            method="POST",
            json={
                "catalog": catalog,
                "description": description,
                "host": host,
                "last_sync": last_sync,
                "last_sync_count": last_sync_count,
                "last_sync_job": last_sync_job,
                "meta": meta,
                "prefix": prefix,
                "presign": presign,
                "presign_ttl": presign_ttl,
                "project": project,
                "recursive_scan": recursive_scan,
                "regex_filter": regex_filter,
                "request_timeout_s": request_timeout_s,
                "schema": schema,
                "status": status,
                "stream_chunk_bytes": stream_chunk_bytes,
                "synchronizable": synchronizable,
                "title": title,
                "token": token,
                "traceback": traceback,
                "use_blob_urls": use_blob_urls,
                "verify_tls": verify_tls,
                "volume": volume,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    DatabricksImportStorage,
                    construct_type(
                        type_=DatabricksImportStorage,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def validate(
        self,
        *,
        catalog: str,
        host: str,
        project: int,
        schema: str,
        volume: str,
        description: typing.Optional[str] = OMIT,
        last_sync: typing.Optional[dt.datetime] = OMIT,
        last_sync_count: typing.Optional[int] = OMIT,
        last_sync_job: typing.Optional[str] = OMIT,
        meta: typing.Optional[typing.Optional[typing.Any]] = OMIT,
        prefix: typing.Optional[str] = OMIT,
        presign: typing.Optional[bool] = OMIT,
        presign_ttl: typing.Optional[int] = OMIT,
        recursive_scan: typing.Optional[bool] = OMIT,
        regex_filter: typing.Optional[str] = OMIT,
        request_timeout_s: typing.Optional[int] = OMIT,
        status: typing.Optional[StatusC5AEnum] = OMIT,
        stream_chunk_bytes: typing.Optional[int] = OMIT,
        synchronizable: typing.Optional[bool] = OMIT,
        title: typing.Optional[str] = OMIT,
        token: typing.Optional[str] = OMIT,
        traceback: typing.Optional[str] = OMIT,
        use_blob_urls: typing.Optional[bool] = OMIT,
        verify_tls: typing.Optional[bool] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> None:
        """
        Validate a specific Databricks Files import storage connection.

        Parameters
        ----------
        catalog : str
            UC catalog name

        host : str
            Databricks workspace base URL (https://...)

        project : int
            A unique integer value identifying this project.

        schema : str
            UC schema name

        volume : str
            UC volume name

        description : typing.Optional[str]
            Cloud storage description

        last_sync : typing.Optional[dt.datetime]
            Last sync finished time

        last_sync_count : typing.Optional[int]
            Count of tasks synced last time

        last_sync_job : typing.Optional[str]
            Last sync job ID

        meta : typing.Optional[typing.Optional[typing.Any]]

        prefix : typing.Optional[str]
            Path under the volume

        presign : typing.Optional[bool]
            Presign not supported; always proxied

        presign_ttl : typing.Optional[int]
            Unused for Databricks; kept for compatibility

        recursive_scan : typing.Optional[bool]
            Perform recursive scan

        regex_filter : typing.Optional[str]
            Regex for filtering objects

        request_timeout_s : typing.Optional[int]

        status : typing.Optional[StatusC5AEnum]

        stream_chunk_bytes : typing.Optional[int]

        synchronizable : typing.Optional[bool]

        title : typing.Optional[str]
            Cloud storage title

        token : typing.Optional[str]

        traceback : typing.Optional[str]
            Traceback report for the last failed sync

        use_blob_urls : typing.Optional[bool]
            Generate blob URLs in tasks

        verify_tls : typing.Optional[bool]
            Verify TLS certificates

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from label_studio_sdk import AsyncLabelStudio

        client = AsyncLabelStudio(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.import_storage.databricks.validate(
                catalog="catalog",
                host="host",
                project=1,
                schema="schema",
                volume="volume",
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            "api/storages/databricks/validate",
            method="POST",
            json={
                "catalog": catalog,
                "description": description,
                "host": host,
                "last_sync": last_sync,
                "last_sync_count": last_sync_count,
                "last_sync_job": last_sync_job,
                "meta": meta,
                "prefix": prefix,
                "presign": presign,
                "presign_ttl": presign_ttl,
                "project": project,
                "recursive_scan": recursive_scan,
                "regex_filter": regex_filter,
                "request_timeout_s": request_timeout_s,
                "schema": schema,
                "status": status,
                "stream_chunk_bytes": stream_chunk_bytes,
                "synchronizable": synchronizable,
                "title": title,
                "token": token,
                "traceback": traceback,
                "use_blob_urls": use_blob_urls,
                "verify_tls": verify_tls,
                "volume": volume,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def get(self, id: int, *, request_options: typing.Optional[RequestOptions] = None) -> DatabricksImportStorage:
        """
        Get a specific Databricks Files import storage connection.

        Parameters
        ----------
        id : int

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        DatabricksImportStorage


        Examples
        --------
        import asyncio

        from label_studio_sdk import AsyncLabelStudio

        client = AsyncLabelStudio(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.import_storage.databricks.get(
                id=1,
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/storages/databricks/{jsonable_encoder(id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    DatabricksImportStorage,
                    construct_type(
                        type_=DatabricksImportStorage,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def delete(self, id: int, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Delete a specific Databricks Files import storage connection.

        Parameters
        ----------
        id : int

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from label_studio_sdk import AsyncLabelStudio

        client = AsyncLabelStudio(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.import_storage.databricks.delete(
                id=1,
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/storages/databricks/{jsonable_encoder(id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def update(
        self,
        id: int,
        *,
        catalog: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        host: typing.Optional[str] = OMIT,
        last_sync: typing.Optional[dt.datetime] = OMIT,
        last_sync_count: typing.Optional[int] = OMIT,
        last_sync_job: typing.Optional[str] = OMIT,
        meta: typing.Optional[typing.Optional[typing.Any]] = OMIT,
        prefix: typing.Optional[str] = OMIT,
        presign: typing.Optional[bool] = OMIT,
        presign_ttl: typing.Optional[int] = OMIT,
        project: typing.Optional[int] = OMIT,
        recursive_scan: typing.Optional[bool] = OMIT,
        regex_filter: typing.Optional[str] = OMIT,
        request_timeout_s: typing.Optional[int] = OMIT,
        schema: typing.Optional[str] = OMIT,
        status: typing.Optional[StatusC5AEnum] = OMIT,
        stream_chunk_bytes: typing.Optional[int] = OMIT,
        synchronizable: typing.Optional[bool] = OMIT,
        title: typing.Optional[str] = OMIT,
        token: typing.Optional[str] = OMIT,
        traceback: typing.Optional[str] = OMIT,
        use_blob_urls: typing.Optional[bool] = OMIT,
        verify_tls: typing.Optional[bool] = OMIT,
        volume: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> DatabricksImportStorage:
        """
        Update a specific Databricks Files import storage connection.

        Parameters
        ----------
        id : int

        catalog : typing.Optional[str]
            UC catalog name

        description : typing.Optional[str]
            Cloud storage description

        host : typing.Optional[str]
            Databricks workspace base URL (https://...)

        last_sync : typing.Optional[dt.datetime]
            Last sync finished time

        last_sync_count : typing.Optional[int]
            Count of tasks synced last time

        last_sync_job : typing.Optional[str]
            Last sync job ID

        meta : typing.Optional[typing.Optional[typing.Any]]

        prefix : typing.Optional[str]
            Path under the volume

        presign : typing.Optional[bool]
            Presign not supported; always proxied

        presign_ttl : typing.Optional[int]
            Unused for Databricks; kept for compatibility

        project : typing.Optional[int]
            A unique integer value identifying this project.

        recursive_scan : typing.Optional[bool]
            Perform recursive scan

        regex_filter : typing.Optional[str]
            Regex for filtering objects

        request_timeout_s : typing.Optional[int]

        schema : typing.Optional[str]
            UC schema name

        status : typing.Optional[StatusC5AEnum]

        stream_chunk_bytes : typing.Optional[int]

        synchronizable : typing.Optional[bool]

        title : typing.Optional[str]
            Cloud storage title

        token : typing.Optional[str]

        traceback : typing.Optional[str]
            Traceback report for the last failed sync

        use_blob_urls : typing.Optional[bool]
            Generate blob URLs in tasks

        verify_tls : typing.Optional[bool]
            Verify TLS certificates

        volume : typing.Optional[str]
            UC volume name

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        DatabricksImportStorage


        Examples
        --------
        import asyncio

        from label_studio_sdk import AsyncLabelStudio

        client = AsyncLabelStudio(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.import_storage.databricks.update(
                id=1,
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/storages/databricks/{jsonable_encoder(id)}",
            method="PATCH",
            json={
                "catalog": catalog,
                "description": description,
                "host": host,
                "last_sync": last_sync,
                "last_sync_count": last_sync_count,
                "last_sync_job": last_sync_job,
                "meta": meta,
                "prefix": prefix,
                "presign": presign,
                "presign_ttl": presign_ttl,
                "project": project,
                "recursive_scan": recursive_scan,
                "regex_filter": regex_filter,
                "request_timeout_s": request_timeout_s,
                "schema": schema,
                "status": status,
                "stream_chunk_bytes": stream_chunk_bytes,
                "synchronizable": synchronizable,
                "title": title,
                "token": token,
                "traceback": traceback,
                "use_blob_urls": use_blob_urls,
                "verify_tls": verify_tls,
                "volume": volume,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    DatabricksImportStorage,
                    construct_type(
                        type_=DatabricksImportStorage,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def sync(
        self, id: int, *, request_options: typing.Optional[RequestOptions] = None
    ) -> DatabricksImportStorage:
        """
        Sync tasks from a Databricks Files import storage.

        Parameters
        ----------
        id : int

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        DatabricksImportStorage


        Examples
        --------
        import asyncio

        from label_studio_sdk import AsyncLabelStudio

        client = AsyncLabelStudio(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.import_storage.databricks.sync(
                id=1,
            )


        asyncio.run(main())
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/storages/databricks/{jsonable_encoder(id)}/sync",
            method="POST",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return typing.cast(
                    DatabricksImportStorage,
                    construct_type(
                        type_=DatabricksImportStorage,  # type: ignore
                        object_=_response.json(),
                    ),
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)
