# This file was auto-generated by Fern from our API Definition.

import typing
from json.decoder import JSONDecodeError

from ...core.api_error import ApiError
from ...core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ...core.http_response import AsyncHttpResponse, HttpResponse
from ...core.jsonable_encoder import jsonable_encoder
from ...core.request_options import RequestOptions
from ...core.unchecked_base_model import construct_type
from ...types.s3import_storage import S3ImportStorage

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class RawS3Client:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def list(
        self,
        *,
        project: int,
        ordering: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[typing.List[S3ImportStorage]]:
        """
        Get a list of all S3 import storage connections.

        Parameters
        ----------
        project : int
            Project ID

        ordering : typing.Optional[str]
            Which field to use when ordering the results.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[typing.List[S3ImportStorage]]

        """
        _response = self._client_wrapper.httpx_client.request(
            "api/storages/s3/",
            method="GET",
            params={
                "ordering": ordering,
                "project": project,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    typing.List[S3ImportStorage],
                    construct_type(
                        type_=typing.List[S3ImportStorage],  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    def create(
        self,
        *,
        regex_filter: typing.Optional[str] = OMIT,
        use_blob_urls: typing.Optional[bool] = OMIT,
        presign: typing.Optional[bool] = OMIT,
        presign_ttl: typing.Optional[int] = OMIT,
        recursive_scan: typing.Optional[bool] = OMIT,
        title: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        project: typing.Optional[int] = OMIT,
        bucket: typing.Optional[str] = OMIT,
        prefix: typing.Optional[str] = OMIT,
        aws_access_key_id: typing.Optional[str] = OMIT,
        aws_secret_access_key: typing.Optional[str] = OMIT,
        aws_session_token: typing.Optional[str] = OMIT,
        aws_sse_kms_key_id: typing.Optional[str] = OMIT,
        region_name: typing.Optional[str] = OMIT,
        s3endpoint: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[S3ImportStorage]:
        """
        Create new S3 import storage

        Parameters
        ----------
        regex_filter : typing.Optional[str]
            Cloud storage regex for filtering objects. You must specify it otherwise no objects will be imported.

        use_blob_urls : typing.Optional[bool]
            Interpret objects as BLOBs and generate URLs. For example, if your bucket contains images, you can use this option to generate URLs for these images. If set to False, it will read the content of the file and load it into Label Studio.

        presign : typing.Optional[bool]
            Presign URLs for download

        presign_ttl : typing.Optional[int]
            Presign TTL in minutes

        recursive_scan : typing.Optional[bool]
            Scan recursively

        title : typing.Optional[str]
            Storage title

        description : typing.Optional[str]
            Storage description

        project : typing.Optional[int]
            Project ID

        bucket : typing.Optional[str]
            S3 bucket name

        prefix : typing.Optional[str]
            S3 bucket prefix

        aws_access_key_id : typing.Optional[str]
            AWS_ACCESS_KEY_ID

        aws_secret_access_key : typing.Optional[str]
            AWS_SECRET_ACCESS_KEY

        aws_session_token : typing.Optional[str]
            AWS_SESSION_TOKEN

        aws_sse_kms_key_id : typing.Optional[str]
            AWS SSE KMS Key ID

        region_name : typing.Optional[str]
            AWS Region

        s3endpoint : typing.Optional[str]
            S3 Endpoint

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[S3ImportStorage]

        """
        _response = self._client_wrapper.httpx_client.request(
            "api/storages/s3/",
            method="POST",
            json={
                "regex_filter": regex_filter,
                "use_blob_urls": use_blob_urls,
                "presign": presign,
                "presign_ttl": presign_ttl,
                "recursive_scan": recursive_scan,
                "title": title,
                "description": description,
                "project": project,
                "bucket": bucket,
                "prefix": prefix,
                "aws_access_key_id": aws_access_key_id,
                "aws_secret_access_key": aws_secret_access_key,
                "aws_session_token": aws_session_token,
                "aws_sse_kms_key_id": aws_sse_kms_key_id,
                "region_name": region_name,
                "s3_endpoint": s3endpoint,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    S3ImportStorage,
                    construct_type(
                        type_=S3ImportStorage,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    def get(self, id: int, *, request_options: typing.Optional[RequestOptions] = None) -> HttpResponse[S3ImportStorage]:
        """
        Get a specific S3 import storage connection.

        Parameters
        ----------
        id : int

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[S3ImportStorage]

        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/storages/s3/{jsonable_encoder(id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    S3ImportStorage,
                    construct_type(
                        type_=S3ImportStorage,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    def delete(self, id: int, *, request_options: typing.Optional[RequestOptions] = None) -> HttpResponse[None]:
        """
        Delete a specific S3 import storage connection.

        Parameters
        ----------
        id : int

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[None]
        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/storages/s3/{jsonable_encoder(id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return HttpResponse(response=_response, data=None)
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    def update(
        self,
        id: int,
        *,
        regex_filter: typing.Optional[str] = OMIT,
        use_blob_urls: typing.Optional[bool] = OMIT,
        presign: typing.Optional[bool] = OMIT,
        presign_ttl: typing.Optional[int] = OMIT,
        recursive_scan: typing.Optional[bool] = OMIT,
        title: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        project: typing.Optional[int] = OMIT,
        bucket: typing.Optional[str] = OMIT,
        prefix: typing.Optional[str] = OMIT,
        aws_access_key_id: typing.Optional[str] = OMIT,
        aws_secret_access_key: typing.Optional[str] = OMIT,
        aws_session_token: typing.Optional[str] = OMIT,
        aws_sse_kms_key_id: typing.Optional[str] = OMIT,
        region_name: typing.Optional[str] = OMIT,
        s3endpoint: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[S3ImportStorage]:
        """
        Update a specific S3 import storage connection.

        Parameters
        ----------
        id : int

        regex_filter : typing.Optional[str]
            Cloud storage regex for filtering objects. You must specify it otherwise no objects will be imported.

        use_blob_urls : typing.Optional[bool]
            Interpret objects as BLOBs and generate URLs. For example, if your bucket contains images, you can use this option to generate URLs for these images. If set to False, it will read the content of the file and load it into Label Studio.

        presign : typing.Optional[bool]
            Presign URLs for download

        presign_ttl : typing.Optional[int]
            Presign TTL in minutes

        recursive_scan : typing.Optional[bool]
            Scan recursively

        title : typing.Optional[str]
            Storage title

        description : typing.Optional[str]
            Storage description

        project : typing.Optional[int]
            Project ID

        bucket : typing.Optional[str]
            S3 bucket name

        prefix : typing.Optional[str]
            S3 bucket prefix

        aws_access_key_id : typing.Optional[str]
            AWS_ACCESS_KEY_ID

        aws_secret_access_key : typing.Optional[str]
            AWS_SECRET_ACCESS_KEY

        aws_session_token : typing.Optional[str]
            AWS_SESSION_TOKEN

        aws_sse_kms_key_id : typing.Optional[str]
            AWS SSE KMS Key ID

        region_name : typing.Optional[str]
            AWS Region

        s3endpoint : typing.Optional[str]
            S3 Endpoint

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[S3ImportStorage]

        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/storages/s3/{jsonable_encoder(id)}",
            method="PATCH",
            json={
                "regex_filter": regex_filter,
                "use_blob_urls": use_blob_urls,
                "presign": presign,
                "presign_ttl": presign_ttl,
                "recursive_scan": recursive_scan,
                "title": title,
                "description": description,
                "project": project,
                "bucket": bucket,
                "prefix": prefix,
                "aws_access_key_id": aws_access_key_id,
                "aws_secret_access_key": aws_secret_access_key,
                "aws_session_token": aws_session_token,
                "aws_sse_kms_key_id": aws_sse_kms_key_id,
                "region_name": region_name,
                "s3_endpoint": s3endpoint,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    S3ImportStorage,
                    construct_type(
                        type_=S3ImportStorage,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    def sync(
        self, id: int, *, request_options: typing.Optional[RequestOptions] = None
    ) -> HttpResponse[S3ImportStorage]:
        """
        Sync tasks from an S3 import storage connection.

        Parameters
        ----------
        id : int
            Storage ID

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[S3ImportStorage]

        """
        _response = self._client_wrapper.httpx_client.request(
            f"api/storages/s3/{jsonable_encoder(id)}/sync",
            method="POST",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    S3ImportStorage,
                    construct_type(
                        type_=S3ImportStorage,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return HttpResponse(response=_response, data=_data)
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    def validate(
        self,
        *,
        id: typing.Optional[int] = OMIT,
        regex_filter: typing.Optional[str] = OMIT,
        use_blob_urls: typing.Optional[bool] = OMIT,
        presign: typing.Optional[bool] = OMIT,
        presign_ttl: typing.Optional[int] = OMIT,
        recursive_scan: typing.Optional[bool] = OMIT,
        title: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        project: typing.Optional[int] = OMIT,
        bucket: typing.Optional[str] = OMIT,
        prefix: typing.Optional[str] = OMIT,
        aws_access_key_id: typing.Optional[str] = OMIT,
        aws_secret_access_key: typing.Optional[str] = OMIT,
        aws_session_token: typing.Optional[str] = OMIT,
        aws_sse_kms_key_id: typing.Optional[str] = OMIT,
        region_name: typing.Optional[str] = OMIT,
        s3endpoint: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> HttpResponse[None]:
        """
        Validate a specific S3 import storage connection.

        Parameters
        ----------
        id : typing.Optional[int]
            Storage ID. If set, storage with specified ID will be updated

        regex_filter : typing.Optional[str]
            Cloud storage regex for filtering objects. You must specify it otherwise no objects will be imported.

        use_blob_urls : typing.Optional[bool]
            Interpret objects as BLOBs and generate URLs. For example, if your bucket contains images, you can use this option to generate URLs for these images. If set to False, it will read the content of the file and load it into Label Studio.

        presign : typing.Optional[bool]
            Presign URLs for download

        presign_ttl : typing.Optional[int]
            Presign TTL in minutes

        recursive_scan : typing.Optional[bool]
            Scan recursively

        title : typing.Optional[str]
            Storage title

        description : typing.Optional[str]
            Storage description

        project : typing.Optional[int]
            Project ID

        bucket : typing.Optional[str]
            S3 bucket name

        prefix : typing.Optional[str]
            S3 bucket prefix

        aws_access_key_id : typing.Optional[str]
            AWS_ACCESS_KEY_ID

        aws_secret_access_key : typing.Optional[str]
            AWS_SECRET_ACCESS_KEY

        aws_session_token : typing.Optional[str]
            AWS_SESSION_TOKEN

        aws_sse_kms_key_id : typing.Optional[str]
            AWS SSE KMS Key ID

        region_name : typing.Optional[str]
            AWS Region

        s3endpoint : typing.Optional[str]
            S3 Endpoint

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        HttpResponse[None]
        """
        _response = self._client_wrapper.httpx_client.request(
            "api/storages/s3/validate",
            method="POST",
            json={
                "id": id,
                "regex_filter": regex_filter,
                "use_blob_urls": use_blob_urls,
                "presign": presign,
                "presign_ttl": presign_ttl,
                "recursive_scan": recursive_scan,
                "title": title,
                "description": description,
                "project": project,
                "bucket": bucket,
                "prefix": prefix,
                "aws_access_key_id": aws_access_key_id,
                "aws_secret_access_key": aws_secret_access_key,
                "aws_session_token": aws_session_token,
                "aws_sse_kms_key_id": aws_sse_kms_key_id,
                "region_name": region_name,
                "s3_endpoint": s3endpoint,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return HttpResponse(response=_response, data=None)
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)


class AsyncRawS3Client:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def list(
        self,
        *,
        project: int,
        ordering: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[typing.List[S3ImportStorage]]:
        """
        Get a list of all S3 import storage connections.

        Parameters
        ----------
        project : int
            Project ID

        ordering : typing.Optional[str]
            Which field to use when ordering the results.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[typing.List[S3ImportStorage]]

        """
        _response = await self._client_wrapper.httpx_client.request(
            "api/storages/s3/",
            method="GET",
            params={
                "ordering": ordering,
                "project": project,
            },
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    typing.List[S3ImportStorage],
                    construct_type(
                        type_=typing.List[S3ImportStorage],  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    async def create(
        self,
        *,
        regex_filter: typing.Optional[str] = OMIT,
        use_blob_urls: typing.Optional[bool] = OMIT,
        presign: typing.Optional[bool] = OMIT,
        presign_ttl: typing.Optional[int] = OMIT,
        recursive_scan: typing.Optional[bool] = OMIT,
        title: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        project: typing.Optional[int] = OMIT,
        bucket: typing.Optional[str] = OMIT,
        prefix: typing.Optional[str] = OMIT,
        aws_access_key_id: typing.Optional[str] = OMIT,
        aws_secret_access_key: typing.Optional[str] = OMIT,
        aws_session_token: typing.Optional[str] = OMIT,
        aws_sse_kms_key_id: typing.Optional[str] = OMIT,
        region_name: typing.Optional[str] = OMIT,
        s3endpoint: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[S3ImportStorage]:
        """
        Create new S3 import storage

        Parameters
        ----------
        regex_filter : typing.Optional[str]
            Cloud storage regex for filtering objects. You must specify it otherwise no objects will be imported.

        use_blob_urls : typing.Optional[bool]
            Interpret objects as BLOBs and generate URLs. For example, if your bucket contains images, you can use this option to generate URLs for these images. If set to False, it will read the content of the file and load it into Label Studio.

        presign : typing.Optional[bool]
            Presign URLs for download

        presign_ttl : typing.Optional[int]
            Presign TTL in minutes

        recursive_scan : typing.Optional[bool]
            Scan recursively

        title : typing.Optional[str]
            Storage title

        description : typing.Optional[str]
            Storage description

        project : typing.Optional[int]
            Project ID

        bucket : typing.Optional[str]
            S3 bucket name

        prefix : typing.Optional[str]
            S3 bucket prefix

        aws_access_key_id : typing.Optional[str]
            AWS_ACCESS_KEY_ID

        aws_secret_access_key : typing.Optional[str]
            AWS_SECRET_ACCESS_KEY

        aws_session_token : typing.Optional[str]
            AWS_SESSION_TOKEN

        aws_sse_kms_key_id : typing.Optional[str]
            AWS SSE KMS Key ID

        region_name : typing.Optional[str]
            AWS Region

        s3endpoint : typing.Optional[str]
            S3 Endpoint

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[S3ImportStorage]

        """
        _response = await self._client_wrapper.httpx_client.request(
            "api/storages/s3/",
            method="POST",
            json={
                "regex_filter": regex_filter,
                "use_blob_urls": use_blob_urls,
                "presign": presign,
                "presign_ttl": presign_ttl,
                "recursive_scan": recursive_scan,
                "title": title,
                "description": description,
                "project": project,
                "bucket": bucket,
                "prefix": prefix,
                "aws_access_key_id": aws_access_key_id,
                "aws_secret_access_key": aws_secret_access_key,
                "aws_session_token": aws_session_token,
                "aws_sse_kms_key_id": aws_sse_kms_key_id,
                "region_name": region_name,
                "s3_endpoint": s3endpoint,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    S3ImportStorage,
                    construct_type(
                        type_=S3ImportStorage,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    async def get(
        self, id: int, *, request_options: typing.Optional[RequestOptions] = None
    ) -> AsyncHttpResponse[S3ImportStorage]:
        """
        Get a specific S3 import storage connection.

        Parameters
        ----------
        id : int

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[S3ImportStorage]

        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/storages/s3/{jsonable_encoder(id)}",
            method="GET",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    S3ImportStorage,
                    construct_type(
                        type_=S3ImportStorage,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    async def delete(
        self, id: int, *, request_options: typing.Optional[RequestOptions] = None
    ) -> AsyncHttpResponse[None]:
        """
        Delete a specific S3 import storage connection.

        Parameters
        ----------
        id : int

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[None]
        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/storages/s3/{jsonable_encoder(id)}",
            method="DELETE",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                return AsyncHttpResponse(response=_response, data=None)
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    async def update(
        self,
        id: int,
        *,
        regex_filter: typing.Optional[str] = OMIT,
        use_blob_urls: typing.Optional[bool] = OMIT,
        presign: typing.Optional[bool] = OMIT,
        presign_ttl: typing.Optional[int] = OMIT,
        recursive_scan: typing.Optional[bool] = OMIT,
        title: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        project: typing.Optional[int] = OMIT,
        bucket: typing.Optional[str] = OMIT,
        prefix: typing.Optional[str] = OMIT,
        aws_access_key_id: typing.Optional[str] = OMIT,
        aws_secret_access_key: typing.Optional[str] = OMIT,
        aws_session_token: typing.Optional[str] = OMIT,
        aws_sse_kms_key_id: typing.Optional[str] = OMIT,
        region_name: typing.Optional[str] = OMIT,
        s3endpoint: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[S3ImportStorage]:
        """
        Update a specific S3 import storage connection.

        Parameters
        ----------
        id : int

        regex_filter : typing.Optional[str]
            Cloud storage regex for filtering objects. You must specify it otherwise no objects will be imported.

        use_blob_urls : typing.Optional[bool]
            Interpret objects as BLOBs and generate URLs. For example, if your bucket contains images, you can use this option to generate URLs for these images. If set to False, it will read the content of the file and load it into Label Studio.

        presign : typing.Optional[bool]
            Presign URLs for download

        presign_ttl : typing.Optional[int]
            Presign TTL in minutes

        recursive_scan : typing.Optional[bool]
            Scan recursively

        title : typing.Optional[str]
            Storage title

        description : typing.Optional[str]
            Storage description

        project : typing.Optional[int]
            Project ID

        bucket : typing.Optional[str]
            S3 bucket name

        prefix : typing.Optional[str]
            S3 bucket prefix

        aws_access_key_id : typing.Optional[str]
            AWS_ACCESS_KEY_ID

        aws_secret_access_key : typing.Optional[str]
            AWS_SECRET_ACCESS_KEY

        aws_session_token : typing.Optional[str]
            AWS_SESSION_TOKEN

        aws_sse_kms_key_id : typing.Optional[str]
            AWS SSE KMS Key ID

        region_name : typing.Optional[str]
            AWS Region

        s3endpoint : typing.Optional[str]
            S3 Endpoint

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[S3ImportStorage]

        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/storages/s3/{jsonable_encoder(id)}",
            method="PATCH",
            json={
                "regex_filter": regex_filter,
                "use_blob_urls": use_blob_urls,
                "presign": presign,
                "presign_ttl": presign_ttl,
                "recursive_scan": recursive_scan,
                "title": title,
                "description": description,
                "project": project,
                "bucket": bucket,
                "prefix": prefix,
                "aws_access_key_id": aws_access_key_id,
                "aws_secret_access_key": aws_secret_access_key,
                "aws_session_token": aws_session_token,
                "aws_sse_kms_key_id": aws_sse_kms_key_id,
                "region_name": region_name,
                "s3_endpoint": s3endpoint,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    S3ImportStorage,
                    construct_type(
                        type_=S3ImportStorage,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    async def sync(
        self, id: int, *, request_options: typing.Optional[RequestOptions] = None
    ) -> AsyncHttpResponse[S3ImportStorage]:
        """
        Sync tasks from an S3 import storage connection.

        Parameters
        ----------
        id : int
            Storage ID

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[S3ImportStorage]

        """
        _response = await self._client_wrapper.httpx_client.request(
            f"api/storages/s3/{jsonable_encoder(id)}/sync",
            method="POST",
            request_options=request_options,
        )
        try:
            if 200 <= _response.status_code < 300:
                _data = typing.cast(
                    S3ImportStorage,
                    construct_type(
                        type_=S3ImportStorage,  # type: ignore
                        object_=_response.json(),
                    ),
                )
                return AsyncHttpResponse(response=_response, data=_data)
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)

    async def validate(
        self,
        *,
        id: typing.Optional[int] = OMIT,
        regex_filter: typing.Optional[str] = OMIT,
        use_blob_urls: typing.Optional[bool] = OMIT,
        presign: typing.Optional[bool] = OMIT,
        presign_ttl: typing.Optional[int] = OMIT,
        recursive_scan: typing.Optional[bool] = OMIT,
        title: typing.Optional[str] = OMIT,
        description: typing.Optional[str] = OMIT,
        project: typing.Optional[int] = OMIT,
        bucket: typing.Optional[str] = OMIT,
        prefix: typing.Optional[str] = OMIT,
        aws_access_key_id: typing.Optional[str] = OMIT,
        aws_secret_access_key: typing.Optional[str] = OMIT,
        aws_session_token: typing.Optional[str] = OMIT,
        aws_sse_kms_key_id: typing.Optional[str] = OMIT,
        region_name: typing.Optional[str] = OMIT,
        s3endpoint: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> AsyncHttpResponse[None]:
        """
        Validate a specific S3 import storage connection.

        Parameters
        ----------
        id : typing.Optional[int]
            Storage ID. If set, storage with specified ID will be updated

        regex_filter : typing.Optional[str]
            Cloud storage regex for filtering objects. You must specify it otherwise no objects will be imported.

        use_blob_urls : typing.Optional[bool]
            Interpret objects as BLOBs and generate URLs. For example, if your bucket contains images, you can use this option to generate URLs for these images. If set to False, it will read the content of the file and load it into Label Studio.

        presign : typing.Optional[bool]
            Presign URLs for download

        presign_ttl : typing.Optional[int]
            Presign TTL in minutes

        recursive_scan : typing.Optional[bool]
            Scan recursively

        title : typing.Optional[str]
            Storage title

        description : typing.Optional[str]
            Storage description

        project : typing.Optional[int]
            Project ID

        bucket : typing.Optional[str]
            S3 bucket name

        prefix : typing.Optional[str]
            S3 bucket prefix

        aws_access_key_id : typing.Optional[str]
            AWS_ACCESS_KEY_ID

        aws_secret_access_key : typing.Optional[str]
            AWS_SECRET_ACCESS_KEY

        aws_session_token : typing.Optional[str]
            AWS_SESSION_TOKEN

        aws_sse_kms_key_id : typing.Optional[str]
            AWS SSE KMS Key ID

        region_name : typing.Optional[str]
            AWS Region

        s3endpoint : typing.Optional[str]
            S3 Endpoint

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        AsyncHttpResponse[None]
        """
        _response = await self._client_wrapper.httpx_client.request(
            "api/storages/s3/validate",
            method="POST",
            json={
                "id": id,
                "regex_filter": regex_filter,
                "use_blob_urls": use_blob_urls,
                "presign": presign,
                "presign_ttl": presign_ttl,
                "recursive_scan": recursive_scan,
                "title": title,
                "description": description,
                "project": project,
                "bucket": bucket,
                "prefix": prefix,
                "aws_access_key_id": aws_access_key_id,
                "aws_secret_access_key": aws_secret_access_key,
                "aws_session_token": aws_session_token,
                "aws_sse_kms_key_id": aws_sse_kms_key_id,
                "region_name": region_name,
                "s3_endpoint": s3endpoint,
            },
            headers={
                "content-type": "application/json",
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return AsyncHttpResponse(response=_response, data=None)
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response.text)
        raise ApiError(status_code=_response.status_code, headers=dict(_response.headers), body=_response_json)
