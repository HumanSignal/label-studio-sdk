imports:
  root: __package__.yml
service:
  auth: false
  base-path: ''
  endpoints:
    list:
      path: /api/prompts/
      method: GET
      auth: true
      docs: |
        Get a list of prompts.
      display-name: List prompts
      response:
        docs: ''
        type: list<root.Prompt>
      examples:
        - response:
            body:
              - title: title
                description: description
                created_by: 1
                created_at: '2024-01-15T09:30:00Z'
                updated_at: '2024-01-15T09:30:00Z'
                organization: 1
                input_fields:
                  - input_fields
                output_classes:
                  - output_classes
                associated_projects:
                  - 1
                skill_name: skill_name
      audiences:
        - public
    create:
      path: /api/prompts/
      method: POST
      auth: true
      docs: |
        Create a new prompt.
      display-name: Create prompt
      request:
        body: root.Prompt
      response:
        docs: ''
        type: root.Prompt
      examples:
        - request:
            title: title
            input_fields:
              - input_fields
            output_classes:
              - output_classes
          response:
            body:
              title: title
              description: description
              created_by: 1
              created_at: '2024-01-15T09:30:00Z'
              updated_at: '2024-01-15T09:30:00Z'
              organization: 1
              input_fields:
                - input_fields
              output_classes:
                - output_classes
              associated_projects:
                - 1
              skill_name: skill_name
      audiences:
        - public
    batch_predictions:
      path: /api/model-run/batch-predictions
      method: POST
      auth: true
      docs: |
        Create a new batch prediction.
      display-name: Create batch predictions
      request:
        name: PromptsBatchPredictionsRequest
        body:
          properties:
            modelrun_id:
              type: optional<integer>
              docs: Model Run ID to associate the prediction with
            results: optional<list<PromptsBatchPredictionsRequestResultsItem>>
      response:
        docs: ''
        type: PromptsBatchPredictionsResponse
      examples:
        - request: {}
          response:
            body:
              detail: detail
      audiences:
        - public
    batch_failed_predictions:
      path: /api/model-run/batch-failed-predictions
      method: POST
      auth: true
      docs: |
        Create a new batch of failed predictions.
      display-name: Create batch of failed predictions
      request:
        name: PromptsBatchFailedPredictionsRequest
        body:
          properties:
            modelrun_id:
              type: optional<integer>
              docs: Model Run ID where the failed predictions came from
            failed_predictions: >-
              optional<list<PromptsBatchFailedPredictionsRequestFailedPredictionsItem>>
      response:
        docs: ''
        type: PromptsBatchFailedPredictionsResponse
      examples:
        - request: {}
          response:
            body:
              detail: detail
      audiences:
        - public
  source:
    openapi: openapi/openapi.yaml
types:
  PromptsBatchPredictionsRequestResultsItem:
    properties:
      task_id:
        type: optional<integer>
        docs: Task ID to associate the prediction with
      output:
        type: optional<map<string, unknown>>
        docs: >
          Prediction output that contains keys from labeling config. Each key
          must be a valid control tag name from the labeling config. For
          example, given the output: ```json {"sentiment": "positive"} ``` it
          will be converted to the internal LS annotation format: ```json {
            "value": {
              "choices": ["positive"]
            },
            "from_name": "label",
            "to_name": "",
            ...
          } ```
      prompt_tokens:
        type: optional<integer>
        docs: Number of tokens in the prompt
      completion_tokens:
        type: optional<integer>
        docs: Number of tokens in the completion
      prompt_cost_usd:
        type: optional<float>
        docs: Cost of the prompt (in USD)
      completion_cost_usd:
        type: optional<float>
        docs: Cost of the completion (in USD)
      total_cost_usd:
        type: optional<float>
        docs: Total cost of the inference (in USD)
    source:
      openapi: openapi/openapi.yaml
  PromptsBatchPredictionsResponse:
    properties:
      detail: optional<string>
    source:
      openapi: openapi/openapi.yaml
  PromptsBatchFailedPredictionsRequestFailedPredictionsItem:
    properties:
      task_id:
        type: optional<integer>
        docs: Task ID to associate the prediction with
      error_type:
        type: optional<string>
        docs: Type of error (e.g. "Timeout", "Rate Limit", etc)
      message:
        type: optional<string>
        docs: Error message details
    source:
      openapi: openapi/openapi.yaml
  PromptsBatchFailedPredictionsResponse:
    properties:
      detail: optional<string>
    source:
      openapi: openapi/openapi.yaml
